---
title: "ДЗ6_Галимова_1"
author: "Adelya Galimova"
date: "2025-05-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
library(magrittr)
library(openxlsx)
library(pander)
library(rio)
library(MASS)
library(factoextra)
```

# Проведем линейный дискриминантный анализ на массиве данных об индексе качества жизни (используем результаты kmeans из прошлого ДЗ).

### Сначала импорт данных:

```{r, message=FALSE, warning=FALSE}
df <- read.xlsx('russian_regions.xlsx', sheet = 'Data')
```

```{r, echo = FALSE}
desc <- read.xlsx('russian_regions.xlsx', sheet = 'Description of data')
kbl(desc, caption = "Таблица 1. Описание данных", booktabs = T, 
    col.names = c("Переменная", "Описание переменной")) %>% 
  kable_classic_2(html_font = "Cambria", font_size = 10, full_width = F) %>%
  pack_rows("Зависимая переменная", 1, 1) %>%
  pack_rows("Уровень жизни", 2, 3) %>%
  pack_rows("Медицина", 4, 6) %>%
  pack_rows("Экология", 7, 8) %>%
  pack_rows("Рынок труда", 9, 17) %>%
  pack_rows("Прочее", 18, 18)
```


## Набор непрерывных переменных:
```{r, message=FALSE, warning=FALSE}
df1 <- df[,2:17]
```

### Перед применением кластерного анализа стандартизируем признаки (это обязательная процедура для применения евклидовой метрики):
```{r, message=FALSE, warning=FALSE}
cluster_df <- scale(df1)
print(cluster_df[0:20])
```

### Также наблюдается проблема мультиколлинеарности, выражающаяся в высокой корреляции между признаками, такими как Q_OF_LIFE_INDEX, AVERAGE_INCOME и GRP.

### Их стоит исключить, что мы и сделаем, так как для кластеризации слабо связанных признаков предпочтительна евклидова метрика, а при сильной корреляции лучше применять расстояние Махаланобиса. Кроме того, наличие сильно коррелированных признаков может исказить результаты модели и затруднить интерпретацию кластеров. Поэтому устранение мультиколлинеарности способствует более точному и надежному анализу данных.

```{r, message=FALSE, warning=FALSE}
df12 <- df1[, -c(3:4)]
str(df12)
```

```{r, message=FALSE, warning=FALSE}
df13 <- df12[, -c(8:9)]
str(df13)
```

```{r, message=FALSE, warning=FALSE}
df2 <- df13[, -c(10:11)]
str(df2)
```

```{r, message=FALSE, warning=FALSE}
cluster_df <- as.data.frame(scale(df2))
```
### Для выполнения дискриминантного анализа мы будем использовать результаты, полученные в ходе кластерного анализа. В частности, применим алгоритм k-средних, который позволяет эффективно разделять данные на группы. Этот метод поможет нам лучше понять структуру данных и выявить скрытые закономерности.

```{r, message=FALSE, warning=FALSE}
set.seed(123)
kmeans3 <- kmeans(cluster_df, centers = 4)
kmeans3
```
#### `(between_SS / total_SS =  38.9 %)` - это отношение между суммой квадратов между кластерами и общей суммой квадратов. Это мера того, насколько хорошо кластеризация разделяет данные. Мы наблюдаем, что полученное значение достаточно высоко, что свидетельствует о неплохом разделении данных.
 
### Теперь присоединим вектора значений принадлежности к кластеру к основным данным:

```{r, message=FALSE, warning=FALSE}
cl <- kmeans3$cluster
cluster_df <- as.data.frame(cbind(cluster_df, cl))
```

## Далее сделаем линейный дискриминантный анализ:

#### Разделение выборки на обучающую (2/3) и тестовую (1/3):

```{r, message=FALSE, warning=FALSE}
smpl_size <- floor(2/3 * nrow(cluster_df))

set.seed(123)
train_ind <- sample(seq_len(nrow(cluster_df)), size = smpl_size)

data_train <- as.data.frame(cluster_df[train_ind,])
data_unknown <- as.data.frame(cluster_df[-train_ind,])
```

## Построим дискриминантную функцию:

```{r, message=FALSE, warning=FALSE}
apply(data_train[, -c(11)], 2, function(x) length(unique(x)))
lda_fit <- lda(data_train[, -c(11, 16, 17)], data_train$cl)
lda_fit
```
### Мы построили модель LDA, за исключением 11-й переменной. Линейные дискриминанты LD1, LD2 и LD3 представляют собой линейные комбинации исходных переменных и объясняют `61.41%`, `26.25%` и `12.34%` дисперсии соответственно. Это указывает на то, что первый линейный дискриминант (LD1) наиболее значим для различения классов, поскольку он объясняет наибольшую долю дисперсии.

```{r, message=FALSE, warning=FALSE}
plot(lda_fit)
```

#### Сравнение графиков для LD1, LD2 и LD3 демонстрирует относительный вклад каждого дискриминанта в разделение классов. Если на одном графике точки хорошо отделены, а на другом — сильно перекрываются, это свидетельствует о том, что соответствующий дискриминант имеет большее значение для различения классов. 

#### Что касается выбросов, на графиках видим точки, которые значительно удалены от остальных элементов своего класса. Это может означать, что присутствуют аномальные данные или на что модель LDA не полностью адекватно описывает данные. Как уже упоминалось, LD1 обеспечивает основное разделение классов (согласно предыдущим выводам и графикам), и классы 1 и 4 хорошо отделены от классов 2 и 3.

#### Для наблюдений из тестовой выборки прогноз включает: "class" — информацию о предсказанной группе для каждого наблюдения; "posterior" — матрицу вероятностей принадлежности к каждому классу для каждого наблюдения из тестовой выборки; "x" — значение дискриминантной функции.

```{r, message=FALSE, warning=FALSE}
lda.predict <- predict(lda_fit, data_unknown[,-c(11)])
names(lda.predict)
```

## Диаграмма рассеяния значений дискриминантных функций

```{r, message=FALSE, warning=FALSE}
plot(lda.predict$x[,1], lda.predict$x[,2]) 
text(lda.predict$x[,1], lda.predict$x[,2], cl, cex = 1, pos = 4, col = "blue")
```

### Классы 1 и 2 имеют значительное перекрытие и не отличаются друг от друга. Класс 3, если смотреть на верхний правый угол, может быть более четко отделен. Линейная дискриминантная функция, вероятно, не является оптимальным решением для данного набора данных, поскольку данные не формируют четко разделимых линейных кластеров.

## Таблица соответствия предсказанных классов с исходными:

```{r, message=FALSE, warning=FALSE}
lda.predict$class
table(lda.predict$class, data_unknown[,c("cl")])
summary(lda.predict$class)
```

```{r, message=FALSE, warning=FALSE}
misclass <- function(pred, obs) { tbl <- table(pred, obs)
sum <- colSums(tbl)
dia <- diag(tbl)
msc <- ((sum - dia)/sum) * 100
m.m <- mean(msc)
cat("Classification table:", "\n")
print(tbl)
cat("Misclassification errors:", "\n")
print(round(msc, 2))

print(round(m.m, 2))}

misclass(lda.predict$class, data_unknown[,c("cl")])
```

### Класс 3 был правильно классифицирован в большинстве случаев (18 из 20), в то время как 25% объектов класса 4 были ошибочно отнесены к другим классам. В общем и целом, результаты оставляют желать лучшего: модель показывает плохую производительность для классов 1 и 2, так как все объекты этих классов были неправильно классифицированы.

## Лямбда Уилкса:

```{r, message=FALSE, warning=FALSE}
ldam <- manova(as.matrix(data_unknown[,-c(11)]) ~ lda.predict$class)

summary(ldam, test = "Wilks")
```

### Статистика Уилкса $(Wilks' lambda)$ = `0.038331`, а значение $(Pr(>F))$ равно `5.055e-06`, что значительно ниже любого уровня значимости. Это указывает на наличие статистически значимого различия между группами (классами) в данных. Гипотеза о равенстве средних значений в группах отвергается. Иными словами, модель LDA успешно разделяет данные на группы, и различия между ними являются значительными.